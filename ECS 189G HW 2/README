Noah A. Smith
2/21/08

David Bamman
2/14/14

This package includes scripts for training (train_hmm.pl or train_hmm.py -- both are equivalent), running (viterbi.pl), and evaluating (tag_acc.pl) HMMs for sequence tagging.  Each script is documented on its own.  Here's a toy example to check that things work.

Train a bigram HMM tagger from sections 2-21 of the Penn Treebank:

  # Perl:
  ./train_hmm.pl ptb.2-21.tgs ptb.2-21.txt > my.hmm

  # Python:
  ./train_hmm.py ptb.2-21.tgs ptb.2-21.txt > my.hmm

Run the Viterbi algorithm to tag some data:

  ./viterbi.pl my.hmm < ptb.22.txt > my.out

Evaluate:

  ./tag_acc.pl ptb.22.tgs my.out

Output should be as follows, modulo floating point differences:

error rate by word:      0.0540917815389984 (2170 errors out of 40117)
error rate by sentence:  0.655882352941176 (1115 errors out of 1700)


20%:
error rate by word:      0.0886157987885435 (3555 errors out of 40117)
error rate by sentence:  0.758823529411765 (1290 errors out of 1700)

40:
error rate by word:      0.0689732532342897 (2767 errors out of 40117)
error rate by sentence:  0.714705882352941 (1215 errors out of 1700)

60:
error rate by word:      0.0614203454894434 (2464 errors out of 40117)
error rate by sentence:  0.687647058823529 (1169 errors out of 1700)

80%:
error rate by word:      0.0572325946606177 (2296 errors out of 40117)
error rate by sentence:  0.665294117647059 (1131 errors out of 1700)

100%
error rate by word:      0.0540917815389984 (2170 errors out of 40117)
error rate by sentence:  0.655882352941176 (1115 errors out of 1700)